(1) Build docker image via Quay.io
	#docker build -t 192.168.200.64:5000/drama/zookeeper:v1 .
	#docker push 192.168.200.64:5000/drama/zookeeper:v1
(2) Create kubernetes pods & services
	kubectl create -f zookeeper.yaml [--namespace=xxx]
(3) Teardown
	./teardown.sh [--namespace=xxx]

Note:
(1) Enable zookeeper cluster nodes' intra communication via 3 services (1 service for 1 pod: zookeeper-1, zookeeper-2, and zookeeper-3)
(2) Use "zookeeper" service as a load balancer for clients. It doesn't matter on which zookeeper pod the clients are connected in a zookeeper cluster.
(3) Since pod name (== hostname), which is generated by k8s replication controller, won't be fixed, we use "service" name (e.g. zookeeper-1, zookeeper-2, zookeeper-3) rather than hostname in zoo.cfg.  
(4) Conceptually, the 3 services (zookeeper-1, zookeeper-2, zookeeper-3) could be headless services, but it doesn't work well in this case (unless delayed the startup of docker program within the pod, so that skydns can add IP entries for these just created pods)   
(5) Resilience verified OK:
  -- follower crash: container (docker stop), pod (kubectl stop), vm (reboot) 
  -- leader crash: container (docker stop), pod (kubectl stop), vm (reboot)
(6) Cluster isolation verified OK:
  -- namespaces under the same user context 
     [current-context: default]
     kubectl create -f zookeeper.yaml --namespace=default
     kubectl create -f zookeeper.yaml --namespace=staging
  -- namespaces under different user context 
     kubectl config use-context develop
     [current-context: develop]
     kubectl create -f zookeeper.yaml --namespace=develop
  Fully isolated among above 3 namespaces

TODO:
(1) Kubernetes 1.0.x doesn't support emptyDir volumes for containers running as non-root (it's commit in master branch, not v1.0.0 branch, refer to https://github.com/kubernetes/kubernetes/pull/9384 & https://github.com/kubernetes/kubernetes/issues/12627). Use root rather than zookeeper user instead at this moment.
(2) Try to spread zookeeper pods (created by different replication controllers, but using same service for external clients) on different nodes by experimenting kube-scheduler.service with policy configuration. 
    -- https://github.com/kubernetes/kubernetes/blob/master/docs/admin/kube-scheduler.md
    -- https://docs.openshift.org/latest/admin_guide/scheduler.html#use-cases 
